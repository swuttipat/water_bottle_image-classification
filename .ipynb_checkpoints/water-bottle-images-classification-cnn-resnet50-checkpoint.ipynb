{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='back_to_top'></a>\n",
    "# Bottle Water Images Classification-CNN & ResNet50\n",
    "\"Classifying Water Bottle Images Based on Water Level Using Machine Learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Created By**: Wuttipat S. <br>\n",
    "**Created Date**: 2023-02-10 <br>\n",
    "**Status**: <span style=\"color:green\">Completed</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update: \n",
    "- **Version6**\n",
    "    - Perform error analysis, fixed code error\n",
    "    - Bring the pre-trained model for performance comparing\n",
    "- **Version5**\n",
    "    - Train and run model with resampling set, which return accucary\n",
    "        - 49% with GridSearchCV (Further tune)\n",
    "- **Version4**\n",
    "    - Add Data Resampling\n",
    "    - Add load images included .*png* file\n",
    "- **Version3**\n",
    "    - Add confusion matrix plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "### 1. [Introduction](#introduction)\n",
    "- [Project Objective](#project_objective)\n",
    "- [Dataset Description](#dataset_description)\n",
    "- [About this directory](#about_this_directory) \n",
    "\n",
    "### 2. [My Tasks](#my_tasks)\n",
    "### 3. [Importing Data from the Directory](#load_dataset) \n",
    "### 4. [Data Preprocessing](#data_preprocessing) \n",
    "1. [Data Augmentation & Data Resampling](#data_augmentation)\n",
    "2. [Nomalizing images value](#nomalizing_images_value)\n",
    "3. [Convert the labels into one-hot encoder array](#convert_the_labels_into_one_hot_encoder_array)\n",
    "\n",
    "### 5. [Machine Learning Model](#machine_learning_model) \n",
    "* [CNN model](#cnn_model)\n",
    "* [Modified pre-trained model(ResNet50)](#resnet50)\n",
    "\n",
    "### 6. [Hyperparameter Tuning](#hyperparameter_tuning)\n",
    "- [GridSearchCV](#gridsearchcv)\n",
    "\n",
    "##  [Note](#note)\n",
    "\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='introduction'></a>\n",
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='project_objective'></a>\n",
    "## Project Objective: \n",
    "The main objective of this project is to develop a machine learning model that can accurately classify water bottle images based on their water levels. The model will be trained on a dataset of water bottle images, with each image being labeled as Full water level, Half water level, or Overflowing. The goal is to develop a model that can accurately classify a given water bottle image based on its water level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataset_description'></a>\n",
    "## Dataset Description: \n",
    "The dataset consists of water bottle images that have been classified based on the level of water inside the bottle. There are three categories of images: Full water level, Half water level, and Overflowing. Each category contains a number of images of water bottles with the corresponding water level. The purpose of the dataset is to be used for an image classification problem, where a machine learning model is trained to classify the water level of a given water bottle image.\n",
    "\n",
    "The dataset is intended to be used for training and testing a machine learning model for image classification. The model will be trained on the provided images, with each image being labeled as either Full water level, Half water level, or Overflowing. The goal of the model is to accurately classify a given water bottle image based on its water level.\n",
    "\n",
    "The dataset consists of a number of water bottle images, each of which has been classified based on the water level inside the bottle. The images in the Full water level category show water bottles with the maximum possible amount of water inside, while the images in the Half water level category show water bottles with roughly half the maximum amount of water inside. The images in the Overflowing category show water bottles with more water inside than the maximum capacity of the bottle, resulting in water spilling out.\n",
    "\n",
    "The dataset is likely to be useful for a variety of applications, such as developing automated systems for monitoring and managing water levels in containers or for use in a general image classification problem. The dataset may also be useful for research purposes, as it allows for the development and testing of machine learning models for image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='about_this_directory'></a>\n",
    "### About this directory\n",
    "\"This folder contains 308 images of water bottles with full water levels. The images show a variety of water bottle sizes and shapes, and are captured from a range of angles. The water bottles are made of plastic and are in good condition. These images could be useful for training a machine learning model to recognize full water levels in water bottles.\"\n",
    "#### The dataset contains with 3 folder:\n",
    "1. Full Water Level - 308 images of full water bottle\n",
    "2. Half water lavel - 139 images of half water bottle\n",
    "3. Overflowing - 39 images of overflowing bottle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<a id='my_tasks'></a>\n",
    "## My Tasks - Image Classification Project\n",
    "\n",
    "\n",
    "1. Create, train, and validate **CNN** model for water bottle images classification:\n",
    "    - Load dataset of water bottle images, and split it into training, validation, and testing sets.\n",
    "    - Design a convolutional neural network (CNN) architecture that is suitable for image classification, and implement it using a deep learning TensorFlow.\n",
    "    - Train the CNN using the training set, and validate it using the validation set to check for overfitting.\n",
    "    - Evaluate the trained model using the testing set, and report its accuracy and other relevant metrics.\n",
    "\n",
    "2. Conduct the pre-trained model **ResNet50** into comparison with prior created model:\n",
    "    - Load the pre-trained ResNet50 model into the notebook and modify it to suit our specific image classification problem.\n",
    "    - Train the modified ResNet50 model using the same training set as the prior created model.\n",
    "    - Evaluate the performance of the modified ResNet50 model on the validation and testing sets, and compare it to the prior created model.\n",
    "\n",
    "3. Build the **GridSearchCV** searching through optimize parameter for the model:\n",
    "    - Choose the relevant hyperparameters that can be tuned for the CNN models, such as the learning rate, batch size, and optimizer algorithm.\n",
    "    - Implement a grid search to exhaustively search through the hyperparameter space and find the optimal combination of hyperparameters that yields the best performance on the validation set.\n",
    "    - Train the CNN model with the optimized hyperparameters and evaluate its performance on the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='load_dataset'></a>\n",
    "## Importing Data from the Directory\n",
    "I started by importing the data from the directory. Using the `OS` module in python to access the directory and its sub-directories. Then use the `OpenCV` library to read the image files and convert them into arrays that can be processed by the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:02:49.509126Z",
     "iopub.status.busy": "2023-02-22T16:02:49.508093Z",
     "iopub.status.idle": "2023-02-22T16:03:02.672811Z",
     "shell.execute_reply": "2023-02-22T16:03:02.671743Z",
     "shell.execute_reply.started": "2023-02-22T16:02:49.509039Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Hide all warnings\n",
    "\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "image_size = (128, 128)\n",
    "\n",
    "# Access the directory and sub-directories and so on\n",
    "# directory = \"water-bottle-dataset\"\n",
    "directory = \"/kaggle/input/water-bottle-dataset\"\n",
    "\n",
    "# Extract all images file inside the folders and stored them into list\n",
    "for sub_folder in os.listdir(directory):\n",
    "    sub_folder_path = os.path.join(directory, sub_folder)\n",
    "    for sub_sub_folder in os.listdir(sub_folder_path):\n",
    "        sub_sub_folder_path = os.path.join(sub_folder_path, sub_sub_folder)\n",
    "        for image_file in os.listdir(sub_sub_folder_path):\n",
    "            if image_file.endswith(\".jpeg\") or image_file.endswith(\".png\"): # Check if the file ends with either '.jped' or '.png'\n",
    "                image_path = os.path.join(sub_sub_folder_path, image_file)\n",
    "                # Read the image using OpenCV\n",
    "                image = cv2.imread(image_path) #the decoded images stored in **B G R** order.\n",
    "                # Resize the image to a standard size\n",
    "                image = cv2.resize(image, image_size)\n",
    "                # Append the image to the data list\n",
    "                data.append(image)\n",
    "                # Append the label to the labels list\n",
    "                labels.append(sub_folder)\n",
    "\n",
    "# Convert the data and labels lists into numpy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Print the dimension of dataset\n",
    "print(f'data shape:{data.shape}')\n",
    "print(f'labels shape:{labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:03:02.675426Z",
     "iopub.status.busy": "2023-02-22T16:03:02.674814Z",
     "iopub.status.idle": "2023-02-22T16:03:02.887620Z",
     "shell.execute_reply": "2023-02-22T16:03:02.886573Z",
     "shell.execute_reply.started": "2023-02-22T16:03:02.675388Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "See how many numbers of each labels\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.DataFrame({\"label\":labels})\n",
    "df.value_counts().plot(kind='bar')\n",
    "plt.xticks(rotation = 0) # Rotates X-Axis Ticks by 45-degrees\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='data_preprocessing'></a>\n",
    "## Cleaning and Data Preprocessing\n",
    "Now that we have imported the data, and need to clean and preprocess the data so that it can be used to train the machine learning model. The following preprocessing steps will be performed:\n",
    "\n",
    "1. **Generate augmented data**. The augmented data is concatenated with the original data to increase the size of the training data.\n",
    "2. **Resampling** is the process of randomly adding or removing data from the dataset to balance the classes. There are two main resampling techniques:\n",
    "\n",
    "    - Undersampling: Undersampling involves randomly removing data from the majority class so that the number of samples in the majority class is the same as the number of samples in the minority class.\n",
    "\n",
    "    - Oversampling: Oversampling involves randomly replicating data from the minority class so that the number of samples in the minority class is the same as the number of samples in the majority class.\n",
    "    \n",
    "    In this notebook I will use *oversampling*.\n",
    "2. **Normalizing** the pixel values to a range between 0 and 1\n",
    "3. **Converting** the labels into one-hot encoded arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_augmentation'></a>\n",
    "#### 1. Data Augmentation & Data Resampling\n",
    "\n",
    "* In the begining of developing the model I generate images by multiplte them for original dateset, the accuracy given is above 80%. But I realize that not answer I look for, since the majority label of dataset is \"Full Water Level\". My model are overfitting with the training data, In addition the test set also engoving with \"Full Water Level\", thus it typical to return high accuracy score.\n",
    "* Next step I bring a **Data Resampling** to fix the overfitting problem. Trainning and test set are equally labels generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:03:02.889325Z",
     "iopub.status.busy": "2023-02-22T16:03:02.888987Z",
     "iopub.status.idle": "2023-02-22T16:03:21.758383Z",
     "shell.execute_reply": "2023-02-22T16:03:21.757375Z",
     "shell.execute_reply.started": "2023-02-22T16:03:02.889291Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Generate augmented data\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load the data\n",
    "X = data # array of preprocessed data\n",
    "y = labels # array of labels\n",
    "n_gen = 40\n",
    "\n",
    "# Create data generator\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=0, #0\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "# Fit the data generator on the data\n",
    "datagen.fit(X)\n",
    "\n",
    "# Generate augmented data\n",
    "X_augmented, y_augmented = [], []\n",
    "\n",
    "'''\n",
    "1st Option multiple dataset with same ratio\n",
    "'''\n",
    "# # Non resampling\n",
    "# for X_batch, y_batch in datagen.flow(X, y, batch_size=32):\n",
    "#     X_augmented.append(X_batch)\n",
    "#     y_augmented.append(y_batch)\n",
    "#     if len(X_augmented) >= 100: # Setting generated augmented data\n",
    "#         break\n",
    "\n",
    "'''\n",
    "2nd Option resampling with equaly labels ratio\n",
    "'''\n",
    "# With resampling\n",
    "for X_batch, y_batch in datagen.flow(X[:308], y[:308], batch_size=32):\n",
    "    X_augmented.append(X_batch)\n",
    "    y_augmented.append(y_batch)\n",
    "    if len(X_augmented) >= n_gen: # Setting generated augmented data\n",
    "        break\n",
    "        \n",
    "for X_batch, y_batch in datagen.flow(X[308:447], y[308:447], batch_size=32):\n",
    "    X_augmented.append(X_batch)\n",
    "    y_augmented.append(y_batch)\n",
    "    if len(X_augmented) >= n_gen*2.3: # Setting generated augmented data\n",
    "        break\n",
    "        \n",
    "for X_batch, y_batch in datagen.flow(X[447:], y[447:], batch_size=32):\n",
    "    X_augmented.append(X_batch)\n",
    "    y_augmented.append(y_batch)\n",
    "    if len(X_augmented) >= n_gen*4.2: # Setting generated augmented data\n",
    "        break\n",
    "\n",
    "# Concatenate augmented data with original data\n",
    "data = np.concatenate((X, np.concatenate(X_augmented)))\n",
    "labels = np.concatenate((y, np.concatenate(y_augmented)))\n",
    "\n",
    "print(f\"data augmented shape : {data.shape}\")\n",
    "print(f\"labels augmented shape : {labels.shape}\")\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"label\":labels})\n",
    "df.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:03:21.762229Z",
     "iopub.status.busy": "2023-02-22T16:03:21.761021Z",
     "iopub.status.idle": "2023-02-22T16:03:21.942221Z",
     "shell.execute_reply": "2023-02-22T16:03:21.941351Z",
     "shell.execute_reply.started": "2023-02-22T16:03:21.762189Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "See how many numbers of each labels. \n",
    "After I regenerated data.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.DataFrame({\"label\":labels})\n",
    "df.value_counts().plot(kind='bar')\n",
    "plt.xticks(rotation = 0) # Rotates X-Axis Ticks by 45-degrees\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test Split\n",
    "\n",
    "* Although mostly neural network previde the train-test split function for itself.\n",
    "* I want to see a result more visualize by plot a *Confusion matrix* from *Predicted of test* and *True labels of test*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:03:21.943786Z",
     "iopub.status.busy": "2023-02-22T16:03:21.943471Z",
     "iopub.status.idle": "2023-02-22T16:03:22.605335Z",
     "shell.execute_reply": "2023-02-22T16:03:22.604306Z",
     "shell.execute_reply.started": "2023-02-22T16:03:21.943754Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "data = X_train # Split training data\n",
    "labels = y_train # Split training labels\n",
    "\n",
    "X_test = X_test # Test data\n",
    "y_test = y_test # Test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:03:22.607388Z",
     "iopub.status.busy": "2023-02-22T16:03:22.606996Z",
     "iopub.status.idle": "2023-02-22T16:03:22.622633Z",
     "shell.execute_reply": "2023-02-22T16:03:22.621591Z",
     "shell.execute_reply.started": "2023-02-22T16:03:22.607352Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(f'data shape:{data.shape}')\n",
    "print(f'labels shape:{labels.shape}')\n",
    "df = pd.DataFrame({\"label\":labels})\n",
    "print(df.value_counts())\n",
    "print(\"\")\n",
    "print(f'test_date shape:{X_test.shape}')\n",
    "print(f'test_labels shape:{y_test.shape}')\n",
    "df = pd.DataFrame({\"test_labels\":y_test})\n",
    "print(df.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nomalizing_images_value'></a>\n",
    "#### 2. Nomalizing images value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:03:22.624631Z",
     "iopub.status.busy": "2023-02-22T16:03:22.624287Z",
     "iopub.status.idle": "2023-02-22T16:03:22.891805Z",
     "shell.execute_reply": "2023-02-22T16:03:22.890696Z",
     "shell.execute_reply.started": "2023-02-22T16:03:22.624598Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize the pixel values to a range between 0 and 1\n",
    "data = data / 255.0\n",
    "X_test = X_test / 225.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='convert_the_labels_into_one_hot_encoder_array'></a>\n",
    "#### 3. Convert the labels into one-hot encoder array\n",
    "Since model create prediction output as (n, 3) dimension array. Converting labels into same type is require for calcuate the model's accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:03:22.894012Z",
     "iopub.status.busy": "2023-02-22T16:03:22.893616Z",
     "iopub.status.idle": "2023-02-22T16:03:22.902944Z",
     "shell.execute_reply": "2023-02-22T16:03:22.901649Z",
     "shell.execute_reply.started": "2023-02-22T16:03:22.893975Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = labels\n",
    "# Convert the labels into one-hot encoded arrays\n",
    "labels_one_hot = np.zeros((labels.shape[0], 3))\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    if label == \"Full  Water level\":\n",
    "        labels_one_hot[i, 0] = 1\n",
    "    elif label == \"Half water level\":\n",
    "        labels_one_hot[i, 1] = 1\n",
    "    else:\n",
    "        labels_one_hot[i, 2] = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:03:22.904878Z",
     "iopub.status.busy": "2023-02-22T16:03:22.904160Z",
     "iopub.status.idle": "2023-02-22T16:03:22.913508Z",
     "shell.execute_reply": "2023-02-22T16:03:22.912575Z",
     "shell.execute_reply.started": "2023-02-22T16:03:22.904827Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show converted output\n",
    "print(labels_one_hot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:03:22.918955Z",
     "iopub.status.busy": "2023-02-22T16:03:22.918097Z",
     "iopub.status.idle": "2023-02-22T16:03:23.658729Z",
     "shell.execute_reply": "2023-02-22T16:03:23.657933Z",
     "shell.execute_reply.started": "2023-02-22T16:03:22.918921Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Show a sample of images from the dataset\n",
    "'''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "data = data\n",
    "\n",
    "# choose 20 random indices\n",
    "indices = np.random.randint(0, len(data), 20)\n",
    "\n",
    "# Get 20 sample images\n",
    "sample_images = data[indices]\n",
    "\n",
    "# Plot the images\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "for i, img in enumerate(sample_images):\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(labels[indices[i]])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate augmented images files (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:03:23.660082Z",
     "iopub.status.busy": "2023-02-22T16:03:23.659753Z",
     "iopub.status.idle": "2023-02-22T16:03:23.667734Z",
     "shell.execute_reply": "2023-02-22T16:03:23.666631Z",
     "shell.execute_reply.started": "2023-02-22T16:03:23.660051Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Save augmented images to specific directory --- Uncomment to use\n",
    "# create new directory to save augmented images\n",
    "import os\n",
    "\n",
    "# Check existing directory, if not: crate new directory\n",
    "if not os.path.exists(\"augmented_images\"):\n",
    "    os.makedirs(\"augmented_images\")\n",
    "\n",
    "augmented_data = data\n",
    "labels = labels\n",
    "# loop through each image in the augmented data\n",
    "for i, image in enumerate(augmented_data):\n",
    "    # convert the image back to its original form\n",
    "    image = (image).astype(\"uint8\")\n",
    "    \n",
    "    # save the image to the new directory\n",
    "    cv2.imwrite(f\"augmented_images/augmented_{labels[i]}_{i}.jpeg\", image)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='machine_learning_model'></a>\n",
    "## Machine Learning Model\n",
    "Finally, we will build, train, and evaluate machine learning models for the image classification problem. I will use the `Keras` and `TensorFlow` library in Python to build and train the models.\n",
    "\n",
    "Here is a list of layers available in TensorFlow along with a brief explanation about each:\n",
    "\n",
    "- **Dense Layer**: A dense layer is a fully connected layer where every input node is connected to every output node. It is the most basic layer in TensorFlow and is used for constructing deep neural networks.\n",
    "\n",
    "- **Convolutional Layer**: A convolutional layer is used for image classification tasks. It uses filters to extract features from the input data.\n",
    "\n",
    "- **Dropout Layer**: A dropout layer is used to prevent overfitting by randomly dropping out neurons during training.\n",
    "\n",
    "- **Batch Normalization** Layer: A batch normalization layer is used to normalize the inputs to a deep neural network. This helps to improve the training process and prevent overfitting.\n",
    "\n",
    "- **Pooling Layer**: A pooling layer is used to reduce the dimensionality of the input data. It is commonly used in image classification tasks to reduce the size of the input image.\n",
    "\n",
    "- **Flatten Layer**: A flatten layer is used to convert the input data from a high-dimensional array to a one-dimensional array. This is used in image classification tasks to prepare the input data for the fully connected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cnn_model'></a>\n",
    "### CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:03:23.670467Z",
     "iopub.status.busy": "2023-02-22T16:03:23.669707Z",
     "iopub.status.idle": "2023-02-22T16:03:26.201817Z",
     "shell.execute_reply": "2023-02-22T16:03:26.199915Z",
     "shell.execute_reply.started": "2023-02-22T16:03:23.670432Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "# set seed value for randomization\n",
    "# np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Build the model using a Convolutional Neural Network\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(128,128,3)),\n",
    "    keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(2,2),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(2,2),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Conv2D(256, (3,3), activation='relu'),\n",
    "    keras.layers.Conv2D(256, (3,3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(2,2),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1024, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# See an overview of the model architecture and to debug issues related to the model layers.\n",
    "model.summary()\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time() #To show the training time\n",
    "\n",
    "# Train the model\n",
    "\n",
    "# set an early stopping mechanism\n",
    "# set patience to be tolerant against random validation loss increases\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "\n",
    "# history = model.fit(data, labels_one_hot, batch_size=32, epochs=10, validation_split=0.2)\n",
    "history = model.fit(x=data,\n",
    "                    y=labels_one_hot,\n",
    "                    batch_size=256,\n",
    "                    epochs=100,\n",
    "                    validation_split=0.2,)\n",
    "#                     callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Test accuracy: \", max(history.history['val_accuracy']))\n",
    "\n",
    "# Assign the trained model\n",
    "self_train_model = history\n",
    "\n",
    "end_time = time.time() # To show the training time \n",
    "training_time = end_time - start_time\n",
    "print(\"Training time:\", training_time, \"seconds\")\n",
    "\n",
    "self_train_model_time = training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resnet50'></a>\n",
    "### Modified ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:07:37.654943Z",
     "iopub.status.busy": "2023-02-22T16:07:37.654576Z",
     "iopub.status.idle": "2023-02-22T16:11:22.484768Z",
     "shell.execute_reply": "2023-02-22T16:11:22.483351Z",
     "shell.execute_reply.started": "2023-02-22T16:07:37.654907Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import time\n",
    "start_time = time.time() #To show the training time\n",
    "\n",
    "X=data\n",
    "y=labels_one_hot\n",
    "\n",
    "# set seed value for randomization\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load pre-trained ResNet50 model\n",
    "resnet = ResNet50(include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "# Freeze layers in ResNet50 model\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add new classification layers\n",
    "x = Flatten()(resnet.output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(3, activation='softmax')(x)\n",
    "\n",
    "# Create new model\n",
    "model = Model(inputs=resnet.input, outputs=x)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X, y, epochs=100, batch_size=256, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Test accuracy: \", max(history.history['val_accuracy']))\n",
    "\n",
    "# Assign the trained model\n",
    "pre_train_model = history\n",
    "\n",
    "end_time = time.time() # To show the training time \n",
    "training_time = end_time - start_time\n",
    "print(\"Training time:\", training_time, \"seconds\")\n",
    "\n",
    "pre_train_model_time = training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot evalution results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:11:22.486503Z",
     "iopub.status.busy": "2023-02-22T16:11:22.486125Z",
     "iopub.status.idle": "2023-02-22T16:11:22.495015Z",
     "shell.execute_reply": "2023-02-22T16:11:22.494024Z",
     "shell.execute_reply.started": "2023-02-22T16:11:22.486468Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_model_loss_and_acc(model, name):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Assign model to variable 'history'\n",
    "    history = model\n",
    "    \n",
    "    # Set Figure size\n",
    "    plt.figure(figsize=(10,5))\n",
    "    \n",
    "    # Plot the training and validation loss\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history['loss'], label='training loss')\n",
    "    plt.plot(history.history['val_loss'], label='validation loss')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.ylim(0,1.1)\n",
    "\n",
    "\n",
    "\n",
    "    # Plot the training and validation accuracy\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history['accuracy'], label='training accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='validation accuracy')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    plt.ylim(0,1.1)\n",
    "    \n",
    "    plt.suptitle(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:11:22.496836Z",
     "iopub.status.busy": "2023-02-22T16:11:22.496238Z",
     "iopub.status.idle": "2023-02-22T16:11:23.102722Z",
     "shell.execute_reply": "2023-02-22T16:11:23.101786Z",
     "shell.execute_reply.started": "2023-02-22T16:11:22.496796Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model_loss_and_acc(self_train_model, 'Self Train CNNs')\n",
    "plot_model_loss_and_acc(pre_train_model, 'With Pre-trained Model(Resnet50)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:11:23.104564Z",
     "iopub.status.busy": "2023-02-22T16:11:23.104221Z",
     "iopub.status.idle": "2023-02-22T16:11:23.110808Z",
     "shell.execute_reply": "2023-02-22T16:11:23.109887Z",
     "shell.execute_reply.started": "2023-02-22T16:11:23.104530Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Convert np.ndarray(n,3) into List of predicted labels\n",
    "'''\n",
    "def output_converter(model_output):\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    output = model_output\n",
    "\n",
    "    # assume that 'output' is a numpy array of shape (n, 3)\n",
    "    output_labels = ['Full  Water level', 'Half water level', 'Overflowing']\n",
    "    predictions = np.argmax(output, axis=1)\n",
    "    predicted_labels = [output_labels[p] for p in predictions]\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:11:23.113235Z",
     "iopub.status.busy": "2023-02-22T16:11:23.112229Z",
     "iopub.status.idle": "2023-02-22T16:11:23.121907Z",
     "shell.execute_reply": "2023-02-22T16:11:23.121043Z",
     "shell.execute_reply.started": "2023-02-22T16:11:23.113199Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Plot a Heatmap-Crosstab table out of predicted labels and True labels\n",
    "'''\n",
    "def plot_hm_ct(y_true, y_pred): \n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # create a DataFrame from y_true and y_pred\n",
    "    df = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred})\n",
    "\n",
    "    # create cross-tabulation matrix\n",
    "    ctab = pd.crosstab(df['y_true'], df['y_pred'])\n",
    "\n",
    "    # create heatmap using seaborn\n",
    "    sns.heatmap(ctab, annot=True, cmap='Blues', fmt='d')\n",
    "\n",
    "    # add labels and title\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.title('Confusion Matrix')\n",
    "\n",
    "    # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:11:23.123818Z",
     "iopub.status.busy": "2023-02-22T16:11:23.123334Z",
     "iopub.status.idle": "2023-02-22T16:11:23.134780Z",
     "shell.execute_reply": "2023-02-22T16:11:23.133882Z",
     "shell.execute_reply.started": "2023-02-22T16:11:23.123785Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Generate confusion matrix from trained model\n",
    "'''\n",
    "def generate_cf(model, name):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Assign model to variable 'history'\n",
    "    history = model\n",
    "    \n",
    "    # Load output data\n",
    "    y_pred = output_converter(history.model.predict(X_test))\n",
    "    y_true = y_test\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    # create a DataFrame from y_true and y_pred\n",
    "    df = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred})\n",
    "\n",
    "    # create cross-tabulation matrix\n",
    "    ctab = pd.crosstab(df['y_true'], df['y_pred'])\n",
    "\n",
    "    # create heatmap using seaborn\n",
    "    sns.heatmap(ctab, annot=True, cmap='Blues', fmt='d')\n",
    "\n",
    "    # add labels and title\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.title('{} Confusion Matrix'.format(name))\n",
    "\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate accuracy score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(\"{} accuracy score: {}\".format(name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:11:23.136517Z",
     "iopub.status.busy": "2023-02-22T16:11:23.136103Z",
     "iopub.status.idle": "2023-02-22T16:11:26.889753Z",
     "shell.execute_reply": "2023-02-22T16:11:26.888684Z",
     "shell.execute_reply.started": "2023-02-22T16:11:23.136483Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_cf(self_train_model, 'Self Train CNNs')\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "generate_cf(pre_train_model, 'With Pre-trained Model (Resnet50)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:11:26.892033Z",
     "iopub.status.busy": "2023-02-22T16:11:26.891040Z",
     "iopub.status.idle": "2023-02-22T16:11:27.128786Z",
     "shell.execute_reply": "2023-02-22T16:11:27.127882Z",
     "shell.execute_reply.started": "2023-02-22T16:11:26.891994Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Preformance Comparision\n",
    "'''\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({'Model': ['CNN', 'Modified ResNet50'],\n",
    "                   'Accuracy': [max(self_train_model.history['accuracy']), max(pre_train_model.history['accuracy'])],\n",
    "                   'Time(s)': [self_train_model_time, pre_train_model_time]})\n",
    "\n",
    "# create a figure and axis object\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# set the bar width\n",
    "bar_width = 0.35\n",
    "\n",
    "# create a bar plot for the first column on the primary y-axis\n",
    "bar1 = ax.bar(df.index, df['Accuracy'], color='b', width=bar_width, label='Accuracy')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "# create a bar plot for the second column on the secondary y-axis\n",
    "ax2 = ax.twinx()\n",
    "bar2 = ax2.bar(df.index + bar_width, df['Time(s)'], color='r', width=bar_width, label='Time(s)')\n",
    "ax2.set_ylabel('Time(s)')\n",
    "\n",
    "# set the title and x-axis label\n",
    "ax.set_title('Bar Chart with Two Columns')\n",
    "ax.set_xlabel('Index')\n",
    "\n",
    "# set the x-axis ticks and labels\n",
    "ax.set_xticks(df.index + bar_width / 2)\n",
    "ax.set_xticklabels(df['Model'])\n",
    "\n",
    "# add the legend\n",
    "handles, labels = [], []\n",
    "for ax in [ax, ax2]:\n",
    "    for h, l in zip(*ax.get_legend_handles_labels()):\n",
    "        handles.append(h)\n",
    "        labels.append(l)\n",
    "ax.legend(handles, labels, loc='best')\n",
    "\n",
    "# display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='hyperparameter_tuning'></a>\n",
    "## 5. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gridsearchcv'></a>\n",
    "**GridSearchCV** is a technique used in machine learning to tune hyperparameters for a model. It allows us to define a grid of hyperparameters to test, and then it will search over all possible combinations of these hyperparameters to find the best combination for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:11:27.130345Z",
     "iopub.status.busy": "2023-02-22T16:11:27.130022Z",
     "iopub.status.idle": "2023-02-22T16:32:36.091894Z",
     "shell.execute_reply": "2023-02-22T16:32:36.090788Z",
     "shell.execute_reply.started": "2023-02-22T16:11:27.130311Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Hide all warnings\n",
    "\n",
    "import time\n",
    "start_time = time.time() #To show the training time\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "batch_size = [128 ,256]\n",
    "epochs = [50,100]\n",
    "optimizer = ['adam']\n",
    "# optimizer = ['adam', 'rmsprop']\n",
    "# cv = 5 # None mean default (K-fold=5)\n",
    "cv = [(slice(None), slice(None))]\n",
    "\n",
    "\n",
    "# Design Model Layers\n",
    "def create_model(optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), activation='relu', input_shape=(128, 128, 3)))\n",
    "    model.add(Conv2D(32, (3, 3),activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu',padding='same'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu',padding='same'))\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model)\n",
    "\n",
    "param_grid = {'batch_size': batch_size,\n",
    "              'epochs': epochs,\n",
    "              'optimizer': optimizer,}\n",
    "#               'callbacks': [early_stopping]} # Disable callbachs function since we want model run with equal epochs for comparing\n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv) \n",
    "grid_result = grid.fit(data, labels_one_hot, verbose=0)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "end_time = time.time() # To show the training time \n",
    "training_time = end_time - start_time\n",
    "print(\"Training time:\", training_time, \"seconds\")\n",
    "grid_time = training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:32:36.094194Z",
     "iopub.status.busy": "2023-02-22T16:32:36.093811Z",
     "iopub.status.idle": "2023-02-22T16:32:36.109129Z",
     "shell.execute_reply": "2023-02-22T16:32:36.107994Z",
     "shell.execute_reply.started": "2023-02-22T16:32:36.094159Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Overview detailed information about the grid search cross-validation process\n",
    "'''\n",
    "import pandas as pd\n",
    "print(pd.DataFrame(grid_result.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:32:36.111236Z",
     "iopub.status.busy": "2023-02-22T16:32:36.110899Z",
     "iopub.status.idle": "2023-02-22T16:32:37.081332Z",
     "shell.execute_reply": "2023-02-22T16:32:37.080336Z",
     "shell.execute_reply.started": "2023-02-22T16:32:36.111203Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Transform predicted result into list\n",
    "'''\n",
    "output_labels = ['Full  Water level', 'Half water level', 'Overflowing']\n",
    "result = grid.predict(X_test)\n",
    "\n",
    "predicted_labels = list(map(lambda x: output_labels[x], result))\n",
    "# print(predicted_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:32:37.084634Z",
     "iopub.status.busy": "2023-02-22T16:32:37.083938Z",
     "iopub.status.idle": "2023-02-22T16:32:37.340695Z",
     "shell.execute_reply": "2023-02-22T16:32:37.339642Z",
     "shell.execute_reply.started": "2023-02-22T16:32:37.084595Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Plot and confusion metrix\n",
    "'''\n",
    "import seaborn as sns\n",
    "\n",
    "# Load output data\n",
    "y_pred = predicted_labels\n",
    "y_true = y_test\n",
    "\n",
    "# Plot the confusion matrix\n",
    "# create a DataFrame from y_true and y_pred\n",
    "df = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred})\n",
    "\n",
    "# create cross-tabulation matrix\n",
    "ctab = pd.crosstab(df['y_true'], df['y_pred'])\n",
    "\n",
    "# create heatmap using seaborn\n",
    "sns.heatmap(ctab, annot=True, cmap='Blues', fmt='d')\n",
    "\n",
    "# add labels and title\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('GridSerachCV result Confusion Matrix')\n",
    "\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"GridSerachCV accuracy score:{}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:32:37.343524Z",
     "iopub.status.busy": "2023-02-22T16:32:37.342841Z",
     "iopub.status.idle": "2023-02-22T16:32:38.096626Z",
     "shell.execute_reply": "2023-02-22T16:32:38.095798Z",
     "shell.execute_reply.started": "2023-02-22T16:32:37.343486Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Show a prediction of images from the test set\n",
    "'''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "X_test = X_test\n",
    "\n",
    "# choose 20 random indices\n",
    "indices = np.random.randint(0, len(X_test), 20)\n",
    "\n",
    "# Get 20 sample images\n",
    "sample_images = X_test[indices]\n",
    "\n",
    "# Plot the images\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "for i, img in enumerate(sample_images):\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title( y_true[indices[i]] + \"\\n\" + \"Predicted result: \" + \"\\n\"+ y_pred[indices[i]])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "# Summary\n",
    "* CNN model yields higher accuracy while takes slightly same amount of time. \n",
    "* The training loss of CNN model decreased to nearly 0, while the ResNet50 model has more resistance of lowering. Due to the differences in the architectures of the two models. This can be assume that CNN model is overfitting the training data more easier the pre-trained model.\n",
    "* GridSearchCV is possible to perform but consuming enormous time and risk of exceeding the memory. Thus, a better approach is to start with a pre-trained model and fine-tune the model to adapt it to our dataset.\n",
    "* Overall, our CNN model was able to classify the water level of a given water bottle image with a high degree of accuracy (<85%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='note'></a>\n",
    "# Note\n",
    "#### Improving a machine learning model can be achieved by using various techniques such as:\n",
    "* ✅ The techniques that I've applied\n",
    "* ❌ The technoques that I think it not neccesary.\n",
    "* 🤔 I don't know how to apply with codek and still figure out.\n",
    "\n",
    "1. **Feature Engineering**❌: Adding or modifying the features used in the model to better capture the underlying patterns in the data.\n",
    "\n",
    "2. **Model Selection**🤔: Choosing a different machine learning model that is more suitable for the data and the problem being solved.\n",
    "\n",
    "3. **Hyperparameter Tuning**✅: Adjusting the parameters of the machine learning model to improve its performance. This can be done manually or using techniques such as grid search or random search.\n",
    "\n",
    "4. **Ensemble Methods**🤔: Combining multiple models to create a more robust model. This can be done by averaging the predictions of multiple models or by training a separate model to make predictions based on the outputs of other models.\n",
    "\n",
    "5. **Regularization**✅: Adding a penalty term to the loss function to prevent overfitting and improve generalization.\n",
    "\n",
    "6. **Data Augmentation**✅: Increasing the size of the dataset by generating new data samples based on the original data. This can help to prevent overfitting and improve generalization.\n",
    "    - After predicting unseen test set model return an ugly result.\n",
    "    - To solve problem I will try clone an equally proportion labels dataset.\n",
    "\n",
    "7. **Early Stopping**✅: Stopping the training process when the model's performance on the validation set starts to deteriorate. This can prevent overfitting and help to avoid the use of models that are too complex.\n",
    "\n",
    "8. **Transfer Learning**🤔: Reusing pre-trained models to reduce the time and computational resources required to train a new model.\n",
    "9. **Data Resampling**✅: Randomly adding or removing data from the dataset to balance the classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Back to top](#back_to_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
